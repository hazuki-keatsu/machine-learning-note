*This network tries to solve the sophisticated input(Vector Set as Input).*

For example:
> 1. Natural language process
> 2. Voice recognition
> 3. Graph (relations between people or protain structure)

# Sequence Labeling

*The amount of input is as same as output.*

But for example if we want to judge whether the comments for something from someone is negative or positive, we need to input the whole sentence and output a label of negative or positive.

So we need to use the Self-attention to process the whole sentence and let each output word with the context information.

![2025-05-03_15-31-03.png](../assets/2025-05-03_15-31-03.png)

> FC: Fully connected network

The most popular paper about Self-attention is [Attetion is all you need](../papers/Attention%20is%20all%20you%20need.pdf).

# Self-attetion

We need to let each input to decide each output.

![](../assets/PixPin_2025-05-03_15-36-36.png)

How can let the vector get the level of relationality?

![](../assets/PixPin_2025-05-03_15-41-15.png)

![](../assets/PixPin_2025-05-03_15-39-50.png)

**Dot-product is the most common way to get $\alpha$.**

![](../assets/2025-05-03_15-46-36.png)

> The Soft-max can be replaced to any function you want.
 
![](../assets/2025-05-03_15-49-30.png)

$\mathbf{W^v,W^k,W^q}$ are determined by training.

# Computation Realitization

![](../assets/2025-05-03_15-59-24.png)
![](../assets/2025-05-03_16-02-30.png)
![](../assets/2025-05-03_16-04-19.png)
![](../assets/2025-05-03_16-06-09.png)

# Multi-head Self-attention

*Different types of relevance.*

![](../assets/2025-05-03_16-09-35.png)
![](../assets/2025-05-03_16-10-46.png)

# Positional Encoding

- No position information in self-attention.

To let the each position has a unique positional information, you can let $\alpha^i$ plus with vector $e^i$.

![](../assets/2025-05-03_16-14-40.png)

Positional Encoding is a problem needed for research. You can have a try on it.

# Application

## Speech

In the self-attention application of speech, the vector may be too long to process, so we can let the vector computed relevance with the nearby vector to less the volume of computation.

![](../assets/2025-05-03_16-17-45.png)
## Image

![](../assets/2025-05-03_16-21-22.png)

# Self-attention vs. CNN

CNN can be seen as special case of Self-attention.

But due to CNN having less flexible, when the volume of data is too small, CNN will have a better performance than self-attention, and when the data is large, the result is the opposite.

# Self-attention vs. RNN

![](../assets/2025-05-03_16-29-01.png)
> Benefit:
> Self-attention can get the relevance with the other vector easily, while self-attention can be computation parallelly.

That is why we prefer to use self-attention.

But there are still some defeats we need to overcome, such as the bigger volume of self-attention.

![](../assets/2025-05-03_16-41-33.png)
