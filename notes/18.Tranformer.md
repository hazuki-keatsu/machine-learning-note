*Transformer is a type of Sequence-to-Sequence model.* Normally, the input is not as same length as the output.

For example:

1. Speech Recognition
2. Speech Translation
3. Machine Translation
4. Object Detection

## Main Process

![](../assets/2025-05-04_18-18-36.png)
![](../assets/2025-05-04_18-19-09.png)

# Encoder

In Transformer, the encoder part is self-attention.

![](../assets/2025-05-04_18-21-52.png)
![](../assets/2025-05-04_18-23-14.png)
![](../assets/2025-05-04_18-25-56.png)

# Decoder

***Autoregressive** is a type of decoder.*

![](../assets/2025-05-05_15-14-49.png)
![](../assets/2025-05-05_15-16-32.png)
![](../assets/2025-05-05_15-17-19.png)

## Masked Multi-Head Attention

![](../assets/2025-05-05_15-21-47.png)

## Stop at END

![](../assets/2025-05-05_15-27-06.png)

# Non-autoregressive

![](../assets/2025-05-05_15-29-25.png)

The way to decide the length of the output:

- We can use another Classifer to predict the length of the output or set a limitation of the output, give a series of BEGIN and get the word before the END as output.

The advantage of NAT:

- parallel, controllable output length

But NAT is usually worse than AT.

![](../assets/2025-05-05_15-37-33.png)
# Training

![](../assets/2025-05-05_15-52-46.png)

**Teacher forcing**: Input the right answer to Decoder directly. But when Tesing, there is obviously no the right answer given to Decoder to see, so there may be mismatch when training.

# Tips for Training

## Copy Mechanism

*Teach the model the ability to copy the token from the input as the output.*

> For example:
> **Chat-bot**:
> 	User: Hello, I'm *Cloro*.
> 	Machine: How are you, *Cloro*. Nice to meet you.
> **Sumarization**:
> 	Get the main details from the article. (The model need many materials for training)

Having the ability to copy the content from the original input is named after "Pointer Network".

## Guided Attention

Sometimes, there will occur the wrong result that we don't know the reason while training.

Guided Attention is to let us lead the process of attention.

The way to solve the problem:
1. Monotonic Attention
2. Location-aware attention

## Beam Search

*Assume there are only two tokens in the world.*

![](../assets/2025-05-05_16-26-30.png)

But beam search is useful or useless sometimes.

**When you need some creativity, you'd better don't use beam search.**

# Optimizing Evaluation Metrics?

**BLEU Score**: Compare two complete sentences to compute the score.

# Exposure bias

Chain reaction due to wrong outputs from Decoder as inputs.

To solve this, we need to put some wrong data in the ground truth. And this measure is named after Scheduled Sampling.