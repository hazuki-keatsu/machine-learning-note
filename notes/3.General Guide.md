# Framework of ML

Training Step: 
1. function with unknown
2. define loss from training data
3. optimization

If your prediction is not ideal:
Check loss on training data
1. if *large*, 
	1. check model bias -> **make your model complex**
	2. optimization -> ~~Next~~
2. if *small*, check loss on testing data
	1. if *large*, 
		1. check overfitting -> **more training data, data augmentation, make your model simpler**
		2. check mismatch[^3] -> ~~Next~~
	2. if *small*, your prediction is good.

# Solve Optimization Issue
1. Gaining the insights from comparison.
2. Start from shallower networks (or other models), which are easier to optimize.
3. If deeper networks do not obtain smaller loss on **training data**, then there is an optimization issue.

> If with the increase of parameter for model, the loss is increasing. Maybe there is overfitting occurring.

# Solve Overfitting

1. More training data
2. Data augmentation: based on your understanding for the issue, create the additional data.
3. constrained[^1] model
	1. Fewer parameters, sharing parameters (CNN)
	2. Fewer the layers of network
	3. Less the features
	4. Early stopping
	5. Regularization[^2]
	6. Dropout

# How to Make Bias-Complexity Trade-off 

> Don't use different model randomly to find out the fitting model.
> 
> This explains why machine usually beats human on benchmark corpora.

**Cross Validation**: Divide Training Set into Training Set and Validation Set

**N-fold Cross Validation**: 

|           | Training Set |           |
| --------- | ------------ | --------- |
| Train     | Train        | Val       |
| Train     | Val          | Train     |
| Val       | Train        | Train     |
| *Avg mse* | *Avg mse*    | *Avg mse* |

Select the best one.

[^1]: constrain: v. 限制
[^2]: regularization: n. 正则化，正则化通过向损失函数添加一个惩罚项来限制模型复杂度，从而减少过拟合并提高模型在新数据上的性能。
[^3]: mismatch: Your training and testing data have different distribution