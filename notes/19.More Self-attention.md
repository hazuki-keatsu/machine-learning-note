有时候我们训练模型的时候效率起不来，这个问题并不一定通过加快Self-attention，来提升速度。只有当Self-attention支配整个训练过程的时候才会有效。

而当self-attention输入的长度十分长的时候，self-attention才会支配整个训练过程。

所以各种self-attention的变体最初都是为了解决图像处理上。

*Bottleneck：瓶颈*

# Local Attention/Truncated Attention

![](../assets/2025-05-16_16-40-34.png)
这种处理方法就是CNN。

# Stride Attention

![](../assets/2025-05-16_16-41-56.png)

# Global Attention

在原来的序列中加入一个特殊的Token，通过这些Token来收集全局信息。

![](../assets/2025-05-16_16-44-55.png)

# How To Choose

可以任意地选择来实现意想不到的效果。（玄学）

# 数据驱动决定Attention位置

## Clustering

如果query和key比较相近，就将他们分类在同一个Class：

![](../assets/2025-05-16_16-49-00.png)

在query和key相近的地方计算Attention：

![](../assets/2025-05-16_16-50-08.png)

## Sinkhorn Sorting Network

通过学习来决定要在哪些地方计算Attention

![](../assets/2025-05-16_16-56-15.png)

可以让很多的输入序列公用一个Matrix共用一个vector，以减少计算量。

同时也可以不用计算所有的value和key，只挑一些有代表性的来进行计算，得到一个局部的patten，用这个局部的patten来计算整体的attention

## Reduce Number of Keys

![](../assets/2025-05-16_17-04-56.png)

**改变矩阵计算的顺序可以加速计算的速度**

## 核函数加快计算Softmax

可以令$\exp{(x \cdot y)} = \phi (x) \cdot \phi (y)$ 来化简运算。

![](../assets/2025-05-16_17-15-29.png)
![](../assets/2025-05-16_17-17-35.png)
![](../assets/2025-05-16_17-19-03.png)
![](../assets/2025-05-16_17-20-05.png)
![](../assets/2025-05-16_17-21-07.png)
![](../assets/2025-05-16_17-21-38.png)
![](../assets/2025-05-16_17-23-09.png)
![](../assets/2025-05-16_17-23-22.png)
![](../assets/2025-05-16_17-24-10.png)

*长脑子；了e*

# Synthesizer

![](../assets/2025-05-16_17-26-54.png)

直接用Network的参数作为Attention Matrix，不使用q和k产生。

# Try to Attention-free

# Summary

- 人类自己的知识
	- Local Attention，Big Bird
- Clustering
	- Reformer
- 通过学习
	- Sinkforn
- 有代表性的Key
	- Linformer
- 使用k，q来生成Attention Matrix
	- Linear Transformer，Performer
- 新的框架
	- Synthesizer

![](../assets/2025-05-16_17-32-44.png)
